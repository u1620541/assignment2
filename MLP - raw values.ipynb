{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13774108887\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import os.path as op\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#path_to_data = op.relpath(\"/modules/cs342/Assignment2/\")\n",
    "path_to_data = \"./data\"\n",
    "\n",
    "light_curves = pd.read_csv(path_to_data + \"/training_set.csv\")\n",
    "metadata     = pd.read_csv(path_to_data + \"/training_set_metadata.csv\")\n",
    "\n",
    "target = metadata[\"target\"]\n",
    "metadata = metadata.drop(\"target\", axis=1)\n",
    "\n",
    "metadata = metadata.drop([\"ra\", \"decl\", \"gal_l\", \"gal_b\", \"distmod\", \"hostgal_specz\"], axis=1)\n",
    "\n",
    "# drop flux error and detected flag for now\n",
    "#light_curves = light_curves.drop([\"flux_err\", \"detected\"], axis=1)\n",
    "\n",
    "\n",
    "print time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81989693642\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tanh_scaler = StandardScaler()\n",
    "tanh_scaler.fit(np.expand_dims(light_curves[\"flux\"].values, 1))\n",
    "scaler_small = MinMaxScaler((-1,1))\n",
    "\n",
    "def process(metadata, light_curves, transform_domain):\n",
    "    flux = light_curves[\"flux\"]\n",
    "    \n",
    "    tanh_flux = np.tanh(transform_domain*tanh_scaler.transform(np.expand_dims(flux,1)))\n",
    "    tanh_curves = light_curves.copy().assign(tanh_flux = tanh_flux)\n",
    "\n",
    "    naive_extraction = tanh_curves.drop(\"mjd\", axis=1).groupby([\"object_id\", \"passband\"]).agg([\"mean\", \"max\", \"min\", \"std\"]).unstack(\"passband\")\n",
    "    X = np.append(metadata.values, naive_extraction, axis=1)\n",
    "    X = scaler_small.fit_transform(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "X = process(metadata, light_curves, 11)\n",
    "y = target.values\n",
    "\n",
    "print time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.008, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(70, 50, 50, 30), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier((70,50,50,30), alpha=0.008, activation=\"tanh\")\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>ddf</th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>hostgal_photoz_err</th>\n",
       "      <th>mwebv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3193</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6323</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6533</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4617</td>\n",
       "      <td>0.0122</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id  ddf  hostgal_photoz  hostgal_photoz_err  mwebv\n",
       "0         13    1          0.3193              0.0542  0.019\n",
       "1         14    1          0.6323              0.0179  0.018\n",
       "2         17    1          0.8297              0.0605  0.016\n",
       "3         23    1          0.6533              0.1479  0.023\n",
       "4         34    1          0.4617              0.0122  0.023"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metadata_backup = pd.read_csv(path_to_data + \"/test_set_metadata.csv\").drop([\"ra\", \"decl\", \"gal_l\", \"gal_b\", \"distmod\", \"hostgal_specz\"], axis = 1)\n",
    "test_metadata_backup.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new chunck  3486831 remaining\n",
      "new chunck  3480774 remaining\n",
      "new chunck  3474720 remaining\n",
      "new chunck  3468645 remaining\n",
      "new chunck  3462570 remaining\n",
      "new chunck  3451032 remaining\n",
      "new chunck  3435408 remaining\n",
      "new chunck  3419801 remaining\n",
      "new chunck  3404182 remaining\n",
      "new chunck  3388574 remaining\n",
      "new chunck  3372969 remaining\n",
      "new chunck  3357321 remaining\n",
      "new chunck  3341680 remaining\n",
      "new chunck  3326056 remaining\n",
      "new chunck  3310427 remaining\n",
      "new chunck  3294801 remaining\n",
      "new chunck  3279192 remaining\n",
      "new chunck  3263543 remaining\n",
      "new chunck  3247905 remaining\n",
      "new chunck  3232267 remaining\n",
      "new chunck  3216640 remaining\n",
      "new chunck  3201019 remaining\n",
      "new chunck  3185405 remaining\n",
      "new chunck  3169765 remaining\n",
      "new chunck  3154135 remaining\n",
      "new chunck  3138500 remaining\n",
      "new chunck  3122851 remaining\n",
      "new chunck  3107221 remaining\n",
      "new chunck  3091598 remaining\n",
      "new chunck  3075956 remaining\n",
      "new chunck  3060323 remaining\n",
      "new chunck  3044706 remaining\n",
      "new chunck  3029064 remaining\n",
      "new chunck  3013431 remaining\n",
      "new chunck  2997815 remaining\n",
      "new chunck  2982185 remaining\n",
      "new chunck  2966551 remaining\n",
      "new chunck  2950919 remaining\n",
      "new chunck  2935288 remaining\n",
      "new chunck  2919649 remaining\n",
      "new chunck  2904028 remaining\n",
      "new chunck  2888396 remaining\n",
      "new chunck  2872767 remaining\n",
      "new chunck  2857126 remaining\n",
      "new chunck  2841494 remaining\n",
      "new chunck  2825864 remaining\n",
      "new chunck  2810218 remaining\n",
      "new chunck  2794572 remaining\n",
      "new chunck  2778958 remaining\n",
      "new chunck  2763345 remaining\n",
      "new chunck  2747709 remaining\n",
      "new chunck  2732095 remaining\n",
      "new chunck  2716459 remaining\n",
      "new chunck  2700822 remaining\n",
      "new chunck  2685190 remaining\n",
      "new chunck  2669596 remaining\n",
      "new chunck  2653939 remaining\n",
      "new chunck  2638323 remaining\n",
      "new chunck  2622670 remaining\n",
      "new chunck  2607063 remaining\n",
      "new chunck  2591453 remaining\n",
      "new chunck  2575843 remaining\n",
      "new chunck  2560229 remaining\n",
      "new chunck  2544625 remaining\n",
      "new chunck  2529023 remaining\n",
      "new chunck  2513430 remaining\n",
      "new chunck  2497833 remaining\n",
      "new chunck  2482199 remaining\n",
      "new chunck  2466586 remaining\n",
      "new chunck  2451002 remaining\n",
      "new chunck  2435373 remaining\n",
      "new chunck  2419742 remaining\n",
      "new chunck  2404105 remaining\n",
      "new chunck  2388490 remaining\n",
      "new chunck  2372863 remaining\n",
      "new chunck  2357233 remaining\n",
      "new chunck  2341601 remaining\n",
      "new chunck  2325948 remaining\n",
      "new chunck  2310330 remaining\n",
      "new chunck  2294695 remaining\n",
      "new chunck  2279057 remaining\n",
      "new chunck  2263400 remaining\n",
      "new chunck  2247795 remaining\n",
      "new chunck  2232180 remaining\n",
      "new chunck  2216524 remaining\n",
      "new chunck  2200912 remaining\n",
      "new chunck  2185290 remaining\n",
      "new chunck  2169640 remaining\n",
      "new chunck  2154027 remaining\n",
      "new chunck  2138427 remaining\n",
      "new chunck  2122794 remaining\n",
      "new chunck  2107171 remaining\n",
      "new chunck  2091549 remaining\n",
      "new chunck  2075916 remaining\n",
      "new chunck  2060298 remaining\n",
      "new chunck  2044651 remaining\n",
      "new chunck  2029025 remaining\n",
      "new chunck  2013387 remaining\n",
      "new chunck  1997751 remaining\n",
      "new chunck  1982127 remaining\n",
      "new chunck  1966493 remaining\n",
      "new chunck  1950858 remaining\n",
      "new chunck  1935222 remaining\n",
      "new chunck  1919597 remaining\n",
      "new chunck  1903986 remaining\n",
      "new chunck  1888359 remaining\n",
      "new chunck  1872728 remaining\n",
      "new chunck  1857087 remaining\n",
      "new chunck  1841472 remaining\n",
      "new chunck  1825842 remaining\n",
      "new chunck  1810188 remaining\n",
      "new chunck  1794566 remaining\n",
      "new chunck  1778938 remaining\n",
      "new chunck  1763313 remaining\n",
      "new chunck  1747699 remaining\n",
      "new chunck  1732071 remaining\n",
      "new chunck  1716425 remaining\n",
      "new chunck  1700784 remaining\n",
      "new chunck  1685135 remaining\n",
      "new chunck  1669509 remaining\n",
      "new chunck  1653873 remaining\n",
      "new chunck  1638252 remaining\n",
      "new chunck  1622598 remaining\n",
      "new chunck  1606954 remaining\n",
      "new chunck  1591306 remaining\n",
      "new chunck  1575661 remaining\n",
      "new chunck  1560010 remaining\n",
      "new chunck  1544397 remaining\n",
      "new chunck  1528780 remaining\n",
      "new chunck  1513162 remaining\n",
      "new chunck  1497531 remaining\n",
      "new chunck  1481916 remaining\n",
      "new chunck  1466270 remaining\n",
      "new chunck  1450678 remaining\n",
      "new chunck  1435068 remaining\n",
      "new chunck  1419436 remaining\n",
      "new chunck  1403824 remaining\n",
      "new chunck  1388153 remaining\n",
      "new chunck  1372533 remaining\n",
      "new chunck  1356921 remaining\n",
      "new chunck  1341284 remaining\n",
      "new chunck  1325656 remaining\n",
      "new chunck  1310037 remaining\n",
      "new chunck  1294423 remaining\n",
      "new chunck  1278822 remaining\n",
      "new chunck  1263207 remaining\n",
      "new chunck  1247569 remaining\n",
      "new chunck  1231923 remaining\n",
      "new chunck  1216300 remaining\n",
      "new chunck  1200672 remaining\n",
      "new chunck  1185044 remaining\n",
      "new chunck  1169413 remaining\n",
      "new chunck  1153783 remaining\n",
      "new chunck  1138156 remaining\n",
      "new chunck  1122540 remaining\n",
      "new chunck  1106931 remaining\n",
      "new chunck  1091296 remaining\n",
      "new chunck  1075661 remaining\n",
      "new chunck  1060012 remaining\n",
      "new chunck  1044388 remaining\n",
      "new chunck  1028781 remaining\n",
      "new chunck  1013140 remaining\n",
      "new chunck  997509 remaining\n",
      "new chunck  981853 remaining\n",
      "new chunck  966224 remaining\n",
      "new chunck  950593 remaining\n",
      "new chunck  934949 remaining\n",
      "new chunck  919351 remaining\n",
      "new chunck  903719 remaining\n",
      "new chunck  888090 remaining\n",
      "new chunck  872469 remaining\n",
      "new chunck  856837 remaining\n",
      "new chunck  841204 remaining\n",
      "new chunck  825576 remaining\n",
      "new chunck  809945 remaining\n",
      "new chunck  794350 remaining\n",
      "new chunck  778706 remaining\n",
      "new chunck  763100 remaining\n",
      "new chunck  747485 remaining\n",
      "new chunck  731844 remaining\n",
      "new chunck  716233 remaining\n",
      "new chunck  700594 remaining\n",
      "new chunck  684981 remaining\n",
      "new chunck  669390 remaining\n",
      "new chunck  653741 remaining\n",
      "new chunck  638119 remaining\n",
      "new chunck  622504 remaining\n",
      "new chunck  606876 remaining\n",
      "new chunck  591264 remaining\n",
      "new chunck  575631 remaining\n",
      "new chunck  559995 remaining\n",
      "new chunck  544371 remaining\n",
      "new chunck  528753 remaining\n",
      "new chunck  513129 remaining\n",
      "new chunck  497510 remaining\n",
      "new chunck  481873 remaining\n",
      "new chunck  466234 remaining\n",
      "new chunck  450597 remaining\n",
      "new chunck  434972 remaining\n",
      "new chunck  419346 remaining\n",
      "new chunck  403709 remaining\n",
      "new chunck  388095 remaining\n",
      "new chunck  372464 remaining\n",
      "new chunck  356808 remaining\n",
      "new chunck  341188 remaining\n",
      "new chunck  325546 remaining\n",
      "new chunck  309939 remaining\n",
      "new chunck  294323 remaining\n",
      "new chunck  278678 remaining\n",
      "new chunck  263034 remaining\n",
      "new chunck  247386 remaining\n",
      "new chunck  231751 remaining\n",
      "new chunck  216104 remaining\n",
      "new chunck  200489 remaining\n",
      "new chunck  184851 remaining\n",
      "new chunck  169213 remaining\n",
      "new chunck  153617 remaining\n",
      "new chunck  137984 remaining\n",
      "new chunck  122392 remaining\n",
      "new chunck  106742 remaining\n",
      "new chunck  91091 remaining\n",
      "new chunck  75478 remaining\n",
      "new chunck  59840 remaining\n",
      "new chunck  44209 remaining\n",
      "new chunck  28551 remaining\n",
      "new chunck  12920 remaining\n",
      "new chunck  1 remaining\n"
     ]
    }
   ],
   "source": [
    "test_metadata = test_metadata_backup.copy()\n",
    "\n",
    "left_over = pd.DataFrame()\n",
    "for partial_light_curves in pd.read_csv(path_to_data + \"/refor_test_data.csv\", chunksize=2000000):\n",
    "    print \"new chunck \",\n",
    "    partial_light_curves = left_over.append(partial_light_curves)\n",
    "    \n",
    "    last_id = partial_light_curves[\"object_id\"].max()\n",
    "    left_over = partial_light_curves.loc[partial_light_curves[\"object_id\"] == last_id]\n",
    "    partial_light_curves = partial_light_curves.drop(left_over.index)\n",
    "\n",
    "    partial_metadata = test_metadata.loc[test_metadata[\"object_id\"] < last_id]\n",
    "    test_metadata = test_metadata.drop(partial_metadata.index)\n",
    "    print len(test_metadata), \"remaining\"\n",
    "    \n",
    "    X = process(partial_metadata, partial_light_curves, 11)\n",
    "\n",
    "    probs = model.predict_proba(np.nan_to_num(X))\n",
    "    # predict class 99 as always 0\n",
    "    probs = np.append(probs, np.zeros((len(probs), 1)), axis=1)\n",
    "    # include ids\n",
    "    probs = np.append(np.array([partial_metadata[\"object_id\"].values]).T, probs, axis=1)\n",
    "    probs = pd.DataFrame(probs)\n",
    "    probs[0] = probs[0].astype(int)\n",
    "    \n",
    "    with open(\"./submissions/submission_MLP_raw.csv\", \"a\") as ofh:\n",
    "        probs.to_csv(ofh, index=False, header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
